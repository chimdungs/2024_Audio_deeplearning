{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7882777,"sourceType":"datasetVersion","datasetId":4626822},{"sourceId":7883354,"sourceType":"datasetVersion","datasetId":4627269},{"sourceId":7884409,"sourceType":"datasetVersion","datasetId":4628075}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport pandas as pd\nimport glob\nfrom collections import defaultdict\nfrom transformers import AutoModelForSeq2SeqLM,AutoTokenizer\nfrom tokenizers import Tokenizer\nfrom transformers import pipeline\nimport warnings\nimport os","metadata":{"execution":{"iopub.status.busy":"2024-03-19T10:37:33.162870Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ko-Bart \n\n사투리 -> 표준어 번역 프로세싱","metadata":{}},{"cell_type":"code","source":"dir = \"/kaggle/input/jeju-labeled-trainset\"\n\nfiles = glob.glob(f\"{dir}/*.json\")\ndf = pd.DataFrame(index=range(2), columns=[\"사투리\",\"표준어\"])\n\nfor file in files:\n    _dict = defaultdict(list)\n    \n    with open(file) as f:\n        json_data = json.load(f)\n        # JSON 파일의 utterance 키 값 아래에 리스트형태로 데이터가 존재했다.\n        data = json_data.get(\"utterance\")\n        \n        for item in data:\n            # standard_form(표준어)와 dialect_form(사투리)가 일치하지 않는다면?\n            standard = item.get(\"standard_form\")\n            dialect = item.get(\"dialect_form\")\n            if standard != dialect:\n                _dict[\"표준어\"].append(standard)\n                _dict[\"사투리\"].append(dialect)\n    \n    # 한 JSON 파일에서 데이터 추출이 끝나면 비어있던 데이터프레임에 추가\n    append_df = pd.DataFrame(_dict)\n    df = (append_df)\n\ndf.to_csv(\"data.tsv\",sep=\"\\t\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\n\nstep1 = re.compile(r'&\\w+&')\nstep2 = re.compile(r'\\(*\\)')\nstep3 = re.compile(r'\\{[^}]*\\}')\nstep4 = re.compile(r'\\-[^}]*\\-')\n\n#예시 문장\ntext1 = \"&name2& 님은 뭐~ {laughing} (()) 힘들 때 -위- 위로가 되어 준 노래 같은 게 있으신가요?\"\ntext2 = \"어~ {반가움} &name123&! (()) 반갑고 ㅋㅋ -오- 오랜만이다\"\n\n# 무식해보이지만 일단 확인해보자!\nfor text in [text1,text2]:\n    after_step1=step1.sub('',text)\n    after_step2=step2.sub('',after_step1)\n    after_step3=step3.sub('',after_step2)\n    after_step4=step4.sub('',after_step3)\n    \n    print(f\"Text : {text}\")\n    print(f\"Step 1 : {after_step1}\")\n    print(f\"Step 2 : {after_step2}\")\n    print(f\"Step 3 : {after_step3}\")\n    print(f\"Step 4 : {after_step4}\")\n    print(\"\\n\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 데이터 정제","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(f'/kaggle/working/data.tsv',sep='\\t')\n\n# data.tsv를 불러왔더니 nan 값이 들어있어 일단 삭제해줬다.\ndf = df.dropna()\n\n# 아까 지정해뒀던 정규표현식들\nstep1 = re.compile(r'&\\w+&')\nstep2 = re.compile(r'\\(*\\)')\nstep3 = re.compile(r'\\{[^}]*\\}')\nstep4 = re.compile(r'\\-[^}]*\\-')\n\n\ndef make_cleaned_text(text):\n    after_step1=step1.sub('',text)\n    after_step2=step2.sub('',after_step1)\n    after_step3=step3.sub('',after_step2)\n    after_step4=step4.sub('',after_step3)\n\n    return after_step4\n\nfor idx, row in df.iterrows():\n    df.at[idx, '사투리'] = make_cleaned_text(row['사투리'])\n    df.at[idx, '표준어'] = make_cleaned_text(row['표준어'])\n\ndf.to_csv('/kaggle/working/data_fixed.tsv',sep='\\t')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 모델 파라미터 설정","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM,AutoTokenizer\n\n# 모델 학습 후에 가중치 저장할 폴더\nmodel_path='/kaggle/working/saved_model'\n# 모델 초기 가중치 로드할 곳\nmodel_name = \"gogamza/kobart-base-v2\"\n# 데이터셋 파일\ndata_root='/kaggle/working'\n\n# 내가 학습한 모델 가중치의 유무에 따라 분기 처리\nif os.path.exists(f'{model_path}/pytorch_model.bin'):\n    print(\"Use Customized Model\")\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\nelse:\n    print(\"Use Pretrained Model\")\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n\n# Training Arguments\n\nargs = {\n    'num_train_epochs': 10,\n    'per_device_train_batch_size': 32,\n    'per_device_eval_batch_size': 32,\n    'overwrite_output_dir': True,\n    'eval_steps': 3000,\n    'save_steps': 6000,\n    'warmup_steps': 300,\n    'evaluation_strategy': \"steps\",\n    'prediction_loss_only': True,\n    'save_total_limit': 3\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TextStyleTransferDataset(Dataset):\n    def __init__(self, df,tokenizer):\n        self.df = df\n        self.tokenizer = tokenizer\n    \n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n        row=self.df.iloc[index]\n        text1=row[0]\n        text2=row[1]\n        target_style_name = '표준어'\n\t\t\n        # Tokenizer를 허깅페이스의 레포지토리에서 가져왔기 때문에\n        # 테스트 코드를 참고해서 모델 입력 형식을 만들었다.\n        encoder_text = f\"{target_style_name} 말투로 변환:{text1}\"\n        decoder_text = f\"{text2}{self.tokenizer.eos_token}\"\n        model_inputs = self.tokenizer(encoder_text, max_length=64, truncation=True)\n\n        with self.tokenizer.as_target_tokenizer():\n            labels = tokenizer(decoder_text, max_length=64, truncation=True)\n        model_inputs['labels'] = labels['input_ids']\n        del model_inputs['token_type_ids']\n\n        return model_inputs\n\n    # 지난번에 만들어뒀던 데이터셋을 불러와서 데이터프레임을 만들어준다\n    def make_df(data_root):\n        df = pd.read_csv(f'{data_root}/data.tsv',sep='\\t')\n        # 주로 쓰이는 방법 같지는 않지만,, Train Data와 Test Data를 8:2 비율로 나눠준다\n        rate=int(len(df)*0.2)\n        df_train,df_test = df[rate:],df[:rate]\n\n        print(f'Train DataFrame length : {len(df_train)},Test DataFrame length : {len(df_test)}')\n        return df_train,df_test\n    \n    def make_dataset(data_root,tokenizer):\n    df_train,df_test = make_df(data_root)\n\n    train_dataset = TextStyleTransferDataset(df_train,tokenizer)\n    test_dataset = TextStyleTransferDataset(df_test,tokenizer)\n\n        return train_dataset,test_dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 학습","metadata":{}},{"cell_type":"code","source":"from transformers import Seq2SeqTrainingArguments,Seq2SeqTrainer,\\\n                         DataCollatorForSeq2Seq\nwarnings.filterwarnings(\"ignore\")\n\n\ntrain_dataset,test_dataset = TextStyleTransferDataset.make_dataset(\n    data_root, tokenizer\n)\n\n\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer=tokenizer, model=model\n)\n\n\ntraining_args = Seq2SeqTrainingArguments(\n    **args,\n    output_dir=model_path,\n    )\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 모델 학습 진행\ntry:\n    trainer.train()\nexcept Exception as e:\n    print(f\"Failed to train model caused by {e}\")\n  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    trainer.save_model(model_path)\n    print(\"Model saved successfully.\")\nexcept Exception as e:\n    print(f\"Failed to save model caused by {e}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test (Validation)\n\nAI hub에 있는 텍스트 데이터에는 테스트용(라벨 없는거)이 따로 없긴 하다.\n그래서 이전에 그냥 trainset에서 8:2로 split시켜서 validation용으로 퉁치긴 했지만\n일단 나중에 오디오 -> 텍스트(STT)실행하고 나서 어차피 데이터는 바꿔서 껴야되기 때문에 그때 validation도 상황에 맞게 데이터 변경해야함\n","metadata":{}},{"cell_type":"code","source":"pip install deep-translater","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import pipeline\nfrom deep_translator import GoogleTranslator\n\nnlg_pipeline=pipeline('text2text-generation',model=config.model_path,tokenizer=config.model_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_text(pipe, text, num_return_sequences, max_length):\n    target_style_name = \"표준어\"\n    text = f\"{target_style_name} 말투로 변환:{text}\"\n    out = pipe(text, num_return_sequences=num_return_sequences, max_length=max_length)\n    return [x['generated_text'] for x in out]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}